{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CBOW_template.ipynb","version":"0.3.2","provenance":[{"file_id":"1limyu1OgdQBdEH7NINVlpwCtbBuWeS7z","timestamp":1552062058403},{"file_id":"1OyGv4MWZ3X6ZmndPIzjuV74Xn5VZr8US","timestamp":1552009798387},{"file_id":"17tPgWRj2GUfHyBv5CinrbDG5KEk5oMlm","timestamp":1551488342397}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"JNvHnPk21XNZ","colab_type":"text"},"cell_type":"markdown","source":["## CBOW Assignment\n","\n","Team member names:  \n","\n","Complete all of the sections as described below.   Then run all, print to pdf using Chrome, and submit on Gradescope (indicating on your submission the start of each part of the assignment and choosing your team members)."]},{"metadata":{"id":"dA0j7Z06QBRc","colab_type":"code","outputId":"5f00c2f6-b9b1-4a31-8cfd-d45314001b10","executionInfo":{"status":"ok","timestamp":1552001663538,"user_tz":300,"elapsed":1737,"user":{"displayName":"Gregery T Buzzard","photoUrl":"https://lh6.googleusercontent.com/-wsbJkMRzkiI/AAAAAAAAAAI/AAAAAAAAADk/SiV0nNoXynw/s64/photo.jpg","userId":"14140499485883029775"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["import keras\n","keras.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'2.2.4'"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"NAUPA-HzQBRt","colab_type":"text"},"cell_type":"markdown","source":["## Continuous bag of words (CBOW) embedding\n","\n","\n","We'll learn a word embedding using the CBOW framework as described in section 4.2 of <a href=\"https://cs224d.stanford.edu/lecture_notes/notes1.pdf\">these notes</a>.  \n","\n","The word corpus will be the \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. The following code loads the dataset and sets a few parameters for use later.   "]},{"metadata":{"id":"mbpCPjrwQBRv","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import imdb\n","\n","V = 5000 # vocabulary size\n","num_reviews = 5000 # number of reviews to use during training\n","num_test = 200 # number of reviews to use during testing and validation\n","dim = 20 # embedding dimension\n","window_size = 2\n","\n","\n","(train_data_full, train_labels_full), (test_data_full, test_labels_full) = imdb.load_data(num_words=V)\n","\n","train_data = train_data_full[0:num_reviews]\n","test_data = test_data_full[0:num_test]\n","val_data = test_data_full[num_test:2*num_test]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2R5D7xCWQBR0","colab_type":"text"},"cell_type":"markdown","source":["\n","The argument `num_words=V` means that we will only keep the top V most frequently occurring words in the training data. Rare words \n","will be discarded. This allows us to work with vector data of manageable size.\n","\n","The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n","`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\".  The labels will not be used for this assignment. "]},{"metadata":{"id":"2_oOc6zEDlR8","colab_type":"text"},"cell_type":"markdown","source":["Here's some code to decode back to English words:"]},{"metadata":{"id":"1O5lSPXpQBR9","colab_type":"code","colab":{}},"cell_type":"code","source":["class WordIndexManager:\n","  def __init__(self, word_index = []):\n","    self.word_index = word_index\n","    self.reverse_word_index = []\n","    \n","    if not (word_index == []): # Reverse the, mapping integer indices to words\n","      self.reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","  def ind_to_string(self, word_ind):\n","    # Decode a word; note that our indices were offset by 3\n","    # because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n","    return self.reverse_word_index.get(word_ind - 3, '?')\n","\n","  def inds_to_string(self, word_inds):  \n","    # Put a list of decoded words into a string\n","    decoded_review = ' '.join([self.ind_to_string(i) for i in word_inds])\n","    return decoded_review\n","\n","  def create_word_list(self, word_inds):\n","    word_list = []\n","    for ind in word_inds:\n","      word_list.append(self.ind_to_string(ind))\n","    return word_list\n","  \n","# word_index is a dictionary mapping words to an integer index\n","# We create an instance of a class to manage this index\n","WIM = WordIndexManager(imdb.get_word_index())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZO9r11P8A__v","colab_type":"text"},"cell_type":"markdown","source":["## TODO:  \n","Use the code above to print the entire first review (index 0) and also to print the 2nd word (index 1) in the first review."]},{"metadata":{"id":"kB907r5cA_TV","colab_type":"code","colab":{}},"cell_type":"code","source":["# print the entire first review\n","\n","\n","# print the second word of the first review\n","\n","# ** YOUR CODE HERE **"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eDXFjg4DQBSC","colab_type":"text"},"cell_type":"markdown","source":["## TODO:\n","\n","Print the top 20 most common words.  Print them in a table form as in:\n","\n","| Index      | Count | Word    |\n","| :---    |    :----:      |    ---: |\n","| 2      | 122808  |    ?   |\n","| ... | ... |  ...|\n","\n","\n","\n"]},{"metadata":{"id":"RnuHfHVNQBSE","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","# ** YOUR CODE HERE **"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nNttP6i-QBSY","colab_type":"text"},"cell_type":"markdown","source":["## Preparing the data\n","\n","\n","We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. This is done with a data generator.  To use the data generator, call the function to get an instance of the generator, then iterate on that instance.  E.g.,\n","\n","```\n","data_gen = generate_data(input_data, window_size, vocab_size, batch_size)\n","for x,y in data_gen:\n","    do something\n","```\n","\n","\n","##TODO: \n","Create a data generator that takes in train_data, window_size, vocab_size, and batch size as above and returns\n","\n","x: a tensor of size \n","```\n","(batch_size,  2*window_size, vocab_size).\n","```\n","Each vector x[i,j,:] is a one-hot encoding of one of the neighbor words of the central word\n","\n","y: a tensor of size \n","```\n","(batch_size, vocab_size).\n","```\n","Each vector y[i,:] is a one-hot encoding of the central word.  \n","\n","You may shuffle the order of the reviews if you like, except the first review (index 0) should be left in place.  \n","\n","Your generator should loop infinitely.  Each time through the loop should lead to \n","\n","```\n","yield (x, y)\n","```"]},{"metadata":{"id":"NcRFyx5HQBSa","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda\n","import keras.backend as K\n","from keras.preprocessing import sequence\n","\n","\n","def generate_data(corpus, window_size, V, batch_size=16):\n","  # ** YOUR CODE HERE **         \n","            "],"execution_count":0,"outputs":[]},{"metadata":{"id":"bSaVMlnEhCH5","colab_type":"text"},"cell_type":"markdown","source":["## TODO:  \n","Verify your work by running the code below.  It should work without modification given the data generator you've written.  The final line should be \n","\n","\n","```\n","film was brilliant casting :  just \n","```\n","\n"]},{"metadata":{"id":"4DPI1_ztZG9c","colab_type":"code","colab":{}},"cell_type":"code","source":["window_size = 2\n","train_gen = generate_data(train_data, window_size, V)\n","val_gen = generate_data(val_data, window_size, V)\n","test_gen = generate_data(test_data, window_size, V)\n","\n","for bow, output in train_gen:\n","  for i in range(5):\n","    for k in range(bow.shape[1]):\n","        ind = np.nonzero(bow[i,k,:])[0][0]\n","        print(WIM.ind_to_string(ind) + ' ', end=\"\")\n","    ind = np.nonzero(output[i,:])[0][0]\n","    print(':  ' + WIM.ind_to_string(ind) + ' ')\n","  break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A4kFCkil_qWf","colab_type":"text"},"cell_type":"markdown","source":["Here we create a function to save the embedding weights for use with the word2vec package, which allows us to explore the word embeddings easily.  "]},{"metadata":{"id":"onA6r0XO4kR0","colab_type":"code","colab":{}},"cell_type":"code","source":["def save_weights(model, vocab_size=V, dim=dim, filename='vectorsCB.txt'):\n","  f = open(filename ,'w')\n","  f.write('{} {}\\n'.format(vocab_size-1, dim))\n","  vectors = model.get_weights()[0]\n","  for i in range(1,vocab_size):\n","      str_vec = ' '.join(map(str, list(vectors[i, :])))\n","      word = WIM.ind_to_string(i)\n","      f.write('{} {}\\n'.format(word, str_vec))\n","  f.close()\n","  return vectors"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dHX3tZsHQBSe","colab_type":"text"},"cell_type":"markdown","source":["## TODO:  \n","Construct a CBOW model called cbow.   Use dim as the embedding dimension. The final layer should be a softmax activation onto the size of the vocabulary.  Use categorical crossentropy.  Use the cbow.summary() function to display the result.  "]},{"metadata":{"id":"SkjxHTsQUHwS","colab_type":"code","colab":{}},"cell_type":"code","source":["cbow = # ** YOUR CODE HERE **\n","\n","cbow.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fZ6106a6z0CJ","colab_type":"text"},"cell_type":"markdown","source":["Save the untrained weights for comparison.  "]},{"metadata":{"id":"1fL7OSQI4t71","colab_type":"code","colab":{}},"cell_type":"code","source":["weightsUT = save_weights(cbow, filename='untrainedCB.txt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P3zQyeaV0YyM","colab_type":"text"},"cell_type":"markdown","source":["Train the model"]},{"metadata":{"id":"NmLgYkF2zzdI","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","val_steps = 100\n","\n","history = cbow.fit_generator(train_gen,\n","                              steps_per_epoch=1500,\n","                              epochs=30,\n","                              validation_data=val_gen,\n","                              validation_steps=val_steps)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3fttj_3V19dT","colab_type":"text"},"cell_type":"markdown","source":["Plot the training"]},{"metadata":{"id":"3yQVXlfJ18CL","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TS7Ww0MHQBSl","colab_type":"text"},"cell_type":"markdown","source":["Save the trained dictionary"]},{"metadata":{"id":"Iwo7N4945ou1","colab_type":"code","colab":{}},"cell_type":"code","source":["weights = save_weights(cbow, filename='vectorsCB.txt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WC6kdVcQ0jEc","colab_type":"text"},"cell_type":"markdown","source":["Load the word embeddings and compare trained and untrained embeddings.  "]},{"metadata":{"id":"A2dmpE7QiNtL","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","import gensim\n","w2vUT = gensim.models.KeyedVectors.load_word2vec_format('./untrainedCB.txt', binary=False)\n","w2vT = gensim.models.KeyedVectors.load_word2vec_format('./vectorsCB.txt', binary=False)\n","\n","def print_similarities(word, w2vUT=w2vUT, w2vT=w2vT):\n","  print('Nearest words and similarities to \"' + word + '\" ')\n","  print('Untrained similarities\\tTrained similarities\\n')\n","  for item1, item2 in zip(w2vUT.most_similar(positive=[word]), w2vT.most_similar(positive=[word])):\n","    print(\"{:10s}\".format(item1[0]) + ', ' + \"{:.2f}\".format(item1[1]) + '\\t' \n","          + \"{:10s}\".format(item2[0]) + ', ' + \"{:.2f}\".format(item2[1]))\n","  print(' ')\n","\n","print_similarities('movie')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EblmaGqr-REh","colab_type":"code","colab":{}},"cell_type":"code","source":["print_similarities('film')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jh-T5L7b53Cg","colab_type":"code","colab":{}},"cell_type":"code","source":["print_similarities('role')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I2Z9le_MGLKL","colab_type":"code","colab":{}},"cell_type":"code","source":["print('Word pair similarity')\n","print('\\t\\t\\tUntrained\\tTrained')\n","print('film and movie: \\t' + \"{:.2f}\".format(w2vUT.similarity('film', 'movie')) \n","      + '\\t\\t' + \"{:.2f}\".format(w2vT.similarity('film', 'movie')))\n","print('man and woman:   \\t' + \"{:.2f}\".format(w2vUT.similarity('man', 'woman')) \n","      + '\\t\\t' + \"{:.2f}\".format(w2vT.similarity('man', 'woman')))\n","print('plot and talent: \\t' + \"{:.2f}\".format(w2vUT.similarity('plot', 'talent')) \n","      + '\\t\\t' + \"{:.2f}\".format(w2vT.similarity('plot', 'talent')))\n","\n","print(' ')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6e8HEnIgF8y9","colab_type":"text"},"cell_type":"markdown","source":["Use TSNE to plot the two primary component of the embedding.  "]},{"metadata":{"id":"TkatQnWtF8YF","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.manifold import TSNE\n","import plotly.offline as py\n","import plotly.graph_objs as go\n","\n","number_of_words = 1000\n","\n","X_embedded = TSNE(n_components=2).fit_transform(weights[0:number_of_words])\n","word_list = WIM.create_word_list(range(number_of_words))\n","\n","\n","trace = go.Scatter(\n","    x = X_embedded[0:number_of_words,0], \n","    y = X_embedded[0:number_of_words, 1],\n","    mode = 'markers',\n","    text= word_list[0:number_of_words]\n",")\n","\n","layout = dict(title= 'Trained t-SNE 1 vs t-SNE 2 for first 1000 words ',\n","              yaxis = dict(title='t-SNE 2'),\n","              xaxis = dict(title='t-SNE 1'),\n","              hovermode= 'closest')\n","\n","fig = dict(data = [trace], layout= layout)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cT5HU0unGFUP","colab_type":"code","colab":{}},"cell_type":"code","source":["def configure_plotly_browser_state():\n","  import IPython\n","  display(IPython.core.display.HTML('''\n","        <script src=\"/static/components/requirejs/require.js\"></script>\n","        <script>\n","          requirejs.config({\n","            paths: {\n","              base: '/static/base',\n","              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n","            },\n","          });\n","        </script>\n","        '''))\n","  \n","configure_plotly_browser_state()  \n","\n","py.init_notebook_mode()\n","py.iplot(fig)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bf5ghfJkGI7O","colab_type":"code","colab":{}},"cell_type":"code","source":["X_embedded = TSNE(n_components=2).fit_transform(weightsUT[0:number_of_words])\n","word_list = WIM.create_word_list(range(number_of_words))\n","\n","\n","trace = go.Scatter(\n","    x = X_embedded[0:number_of_words,0], \n","    y = X_embedded[0:number_of_words, 1],\n","    mode = 'markers',\n","    text= word_list[0:number_of_words]\n",")\n","\n","layout = dict(title= 'Untrained t-SNE 1 vs t-SNE 2 for first 1000 words ',\n","              yaxis = dict(title='t-SNE 2'),\n","              xaxis = dict(title='t-SNE 1'),\n","              hovermode= 'closest')\n","\n","fig = dict(data = [trace], layout= layout)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u8wtABZfGMRX","colab_type":"code","colab":{}},"cell_type":"code","source":["configure_plotly_browser_state()  \n","\n","py.init_notebook_mode()\n","py.iplot(fig)"],"execution_count":0,"outputs":[]}]}