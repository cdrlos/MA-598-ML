{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 2 - RNN prediction.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"p8r1E-ZWgoPm","colab_type":"code","outputId":"8c8cfcfb-ea1e-499a-cda1-cdcf0aa82810","executionInfo":{"status":"ok","timestamp":1550625459093,"user_tz":300,"elapsed":1727,"user":{"displayName":"Gregery T Buzzard","photoUrl":"https://lh6.googleusercontent.com/-wsbJkMRzkiI/AAAAAAAAAAI/AAAAAAAAADk/SiV0nNoXynw/s64/photo.jpg","userId":"14140499485883029775"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["import keras\n","keras.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'2.2.4'"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"tVNXhrapgoP0","colab_type":"text"},"cell_type":"markdown","source":["## Recurrent neural networks for weather prediction\n","\n","This notebook adapts the code samples found in Chapter 6, Section 3 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n","\n","---\n","\n","We use a GRU together with\n","\n","* *Recurrent dropout*, a specific, built-in way to use dropout to fight overfitting in recurrent layers.\n","\n","In Model 1, we add a skip connection that adds the baseline nonlearning prediction.\n","\n","In Model 2, we add encoded time and date.  "]},{"metadata":{"id":"qjXbEib4LiqF","colab_type":"code","colab":{}},"cell_type":"code","source":["model_num = 1 # 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2lpEeW_bgoP2","colab_type":"text"},"cell_type":"markdown","source":["## A temperature forecasting problem\n","\n","Until now, the only sequence data we have covered has been text data, for instance the IMDB dataset and the Reuters dataset. But sequence \n","data is found in many more problems than just language processing. In all of our examples in this section, we will be playing with a weather \n","timeseries dataset recorded at the Weather Station at the Max-Planck-Institute for Biogeochemistry in Jena, Germany: http://www.bgc-jena.mpg.de/wetter/.\n","\n","In this dataset, fourteen different quantities (such air temperature, atmospheric pressure, humidity, wind direction, etc.) are recorded \n","every ten minutes, over several years. The original data goes back to 2003, but we limit ourselves to data from 2009-2016. This dataset is \n","perfect for learning to work with numerical timeseries. We will use it to build a model that takes as input some data from the recent past (a \n","few days worth of data points) and predicts the air temperature 24 hours in the future."]},{"metadata":{"id":"7A7JSD2tgoP4","colab_type":"text"},"cell_type":"markdown","source":["Let's take a look at the data:"]},{"metadata":{"id":"g2dXPhm7sMxz","colab_type":"code","outputId":"2675679f-527e-4178-c059-453ca04b644c","executionInfo":{"status":"ok","timestamp":1550625488461,"user_tz":300,"elapsed":31070,"user":{"displayName":"Gregery T Buzzard","photoUrl":"https://lh6.googleusercontent.com/-wsbJkMRzkiI/AAAAAAAAAAI/AAAAAAAAADk/SiV0nNoXynw/s64/photo.jpg","userId":"14140499485883029775"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"R6b8z4uwgoP5","colab_type":"code","outputId":"c180f2d1-293f-4650-f92a-1e11a49cd6da","executionInfo":{"status":"ok","timestamp":1550625489865,"user_tz":300,"elapsed":32445,"user":{"displayName":"Gregery T Buzzard","photoUrl":"https://lh6.googleusercontent.com/-wsbJkMRzkiI/AAAAAAAAAAI/AAAAAAAAADk/SiV0nNoXynw/s64/photo.jpg","userId":"14140499485883029775"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["import os\n","\n","data_dir = '/content/gdrive/My Drive/Datasets'\n","fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n","\n","f = open(fname)\n","data = f.read()\n","f.close()\n","\n","lines = data.split('\\n')\n","header = lines[0].split(',')\n","lines = lines[1:]\n","\n","print(header)\n","print(len(lines))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n","420551\n"],"name":"stdout"}]},{"metadata":{"id":"gd8Mo15wFo1P","colab_type":"code","outputId":"75205599-1ad7-45ec-9eb2-195ed33dde31","executionInfo":{"status":"ok","timestamp":1550633588023,"user_tz":300,"elapsed":450,"user":{"displayName":"Gregery T Buzzard","photoUrl":"https://lh6.googleusercontent.com/-wsbJkMRzkiI/AAAAAAAAAAI/AAAAAAAAADk/SiV0nNoXynw/s64/photo.jpg","userId":"14140499485883029775"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["print(lines[1000:1003])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['07.01.2009 22:50:00,997.79,-10.79,262.54,-11.84,91.90,2.68,2.46,0.22,1.53,2.47,1323.57,0.24,0.63,139.40', '07.01.2009 23:00:00,997.88,-10.68,262.64,-11.74,91.80,2.70,2.48,0.22,1.55,2.48,1323.13,0.10,0.50,184.20', '07.01.2009 23:10:00,998.05,-10.99,262.32,-12.14,91.10,2.63,2.40,0.23,1.50,2.40,1324.97,0.20,0.63,146.60']\n"],"name":"stdout"}]},{"metadata":{"id":"L5jLNcUGgoP_","colab_type":"text"},"cell_type":"markdown","source":["Let's convert all of these 420,551 lines of data into a Numpy array:"]},{"metadata":{"id":"hjGJdAw7goQB","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","month_lens = [0,31,28,31,30,31,30,31,31,30,31,30]\n","cum_len = np.cumsum(month_lens)\n","\n","float_data = np.zeros((len(lines), len(header) - 1))\n","time_codes = np.zeros((len(lines), 4))\n","\n","for i, line in enumerate(lines):\n","    # separate the entries and convert all but first to float\n","    entries = line.split(',')\n","    values = [float(x) for x in entries[1:]]\n","    float_data[i, :] = values\n","    \n","    # Get the month and date and convert to unit circle\n","    date = float(entries[0][0:2])\n","    month = int(entries[0][3:5])\n","    year_frac = (date + cum_len[month-1])/365.0\n","    time_codes[i,0] = np.sin(year_frac)\n","    time_codes[i,1] = np.cos(year_frac)\n","    \n","    # Get the hour and minute and convert to unit circle\n","    hour = float(entries[0][11:13])\n","    minute = float(entries[0][14:16])\n","    day_frac = (hour + minute/60.0)/24.0\n","    time_codes[i,2] = np.sin(day_frac)\n","    time_codes[i,3] = np.cos(day_frac)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b81QVldCF0jz","colab_type":"code","outputId":"0fe4b364-e948-4960-d812-7985559400b5","executionInfo":{"status":"ok","timestamp":1550634742544,"user_tz":300,"elapsed":591,"user":{"displayName":"Gregery T Buzzard","photoUrl":"https://lh6.googleusercontent.com/-wsbJkMRzkiI/AAAAAAAAAAI/AAAAAAAAADk/SiV0nNoXynw/s64/photo.jpg","userId":"14140499485883029775"}},"colab":{"base_uri":"https://localhost:8080/","height":218}},"cell_type":"code","source":["print(float_data[1000:1003])\n","print(time_codes[1000:1003, :])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 9.97790e+02 -1.07900e+01  2.62540e+02 -1.18400e+01  9.19000e+01\n","   2.68000e+00  2.46000e+00  2.20000e-01  1.53000e+00  2.47000e+00\n","   1.32357e+03  2.40000e-01  6.30000e-01  1.39400e+02]\n"," [ 9.97880e+02 -1.06800e+01  2.62640e+02 -1.17400e+01  9.18000e+01\n","   2.70000e+00  2.48000e+00  2.20000e-01  1.55000e+00  2.48000e+00\n","   1.32313e+03  1.00000e-01  5.00000e-01  1.84200e+02]\n"," [ 9.98050e+02 -1.09900e+01  2.62320e+02 -1.21400e+01  9.11000e+01\n","   2.63000e+00  2.40000e+00  2.30000e-01  1.50000e+00  2.40000e+00\n","   1.32497e+03  2.00000e-01  6.30000e-01  1.46600e+02]]\n","[[0.01917691 0.99981611 0.81422261 0.58055279]\n"," [0.01917691 0.99981611 0.81823456 0.57488451]\n"," [0.01917691 0.99981611 0.82220706 0.56918851]]\n"],"name":"stdout"}]},{"metadata":{"id":"97qHRCELgoQV","colab_type":"text"},"cell_type":"markdown","source":["## Preparing the data\n","\n","\n","The exact formulation of our problem will be the following: given data going as far back as `lookback` timesteps (a timestep is 10 minutes) \n","and sampled every `steps` timesteps, can we predict the temperature in `delay` timesteps?\n","\n","We will use the following parameter values:\n","\n","* `lookback = 720`, i.e. our observations will go back 5 days.\n","* `steps = 6`, i.e. our observations will be sampled at one data point per hour.\n","* `delay = 144`, i.e. our targets will be 24 hours in the future.\n","\n","To get started, we need to do two things:\n","\n","* Preprocess the data to a format a neural network can ingest. This is easy: the data is already numerical, so we don't need to do any \n","vectorization. However each timeseries in the data is on a different scale (e.g. temperature is typically between -20 and +30, but \n","pressure, measured in mbar, is around 1000). So we will normalize each timeseries independently so that they all take small values on a \n","similar scale.\n","* Write a Python generator that takes our current array of float data and yields batches of data from the recent past, alongside with a \n","target temperature in the future. Since the samples in our dataset are highly redundant (e.g. sample `N` and sample `N + 1` will have most \n","of their timesteps in common), it would be very wasteful to explicitly allocate every sample. Instead, we will generate the samples on the \n","fly using the original data.\n","\n","We preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation. We plan on using the first \n","200,000 timesteps as training data, so we compute the mean and standard deviation only on this fraction of the data:"]},{"metadata":{"id":"_k5_M3cOgoQW","colab_type":"code","colab":{}},"cell_type":"code","source":["mean = float_data[:200000].mean(axis=0)\n","float_data -= mean\n","std = float_data[:200000].std(axis=0)\n","float_data /= std"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U3r_D2SjgoQa","colab_type":"text"},"cell_type":"markdown","source":["\n","Now here is the data generator that we will use. It yields a tuple `(samples, targets)` where `samples` is one batch of input data and \n","`targets` is the corresponding array of target temperatures. It takes the following arguments:\n","\n","* `data`: The original array of floating point data, which we just normalized in the code snippet above.\n","* `lookback`: How many timesteps back should our input data go.\n","* `delay`: How many timesteps in the future should our target be.\n","* `min_index` and `max_index`: Indices in the `data` array that delimit which timesteps to draw from. This is useful for keeping a segment \n","of the data for validation and another one for testing.\n","* `shuffle`: Whether to shuffle our samples or draw them in chronological order.\n","* `batch_size`: The number of samples per batch.\n","* `step`: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."]},{"metadata":{"id":"yxUhBRARgoQc","colab_type":"code","colab":{}},"cell_type":"code","source":["def generator(data, lookback, delay, min_index, max_index,\n","              shuffle=False, batch_size=128, step=6):\n","    if max_index is None:\n","        max_index = len(data) - delay - 1\n","    i = min_index + lookback\n","    while 1:\n","        if shuffle:\n","            rows = np.random.randint(\n","                min_index + lookback, max_index, size=batch_size)\n","        else:\n","            if i + batch_size >= max_index:\n","                i = min_index + lookback\n","            rows = np.arange(i, min(i + batch_size, max_index))\n","            i += len(rows)\n","\n","        samples = np.zeros((len(rows),\n","                           lookback // step,\n","                           data.shape[-1]))\n","        targets = np.zeros((len(rows),))\n","        for j, row in enumerate(rows):\n","            indices = range(rows[j] - lookback, rows[j], step)\n","            samples[j] = data[indices]\n","            targets[j] = data[rows[j] + delay][1]\n","        yield samples, targets"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Eu5fxIqlgoQm","colab_type":"text"},"cell_type":"markdown","source":["\n","Now let's use our abstract generator function to instantiate three generators, one for training, one for validation and one for testing. \n","Each will look at different temporal segments of the original data: the training generator looks at the first 200,000 timesteps, the \n","validation generator looks at the following 100,000, and the test generator looks at the remainder."]},{"metadata":{"id":"5kKT16HrgoQm","colab_type":"code","colab":{}},"cell_type":"code","source":["lookback = 432\n","step = 6\n","delay = 144\n","batch_size = 128\n","\n","if (model_num == 2):  # add the time codes for Model 2\n","  float_data = np.concatenate((float_data, time_codes), axis=1)\n","\n","train_gen = generator(float_data,\n","                      lookback=lookback,\n","                      delay=delay,\n","                      min_index=0,\n","                      max_index=200000,\n","                      shuffle=True,\n","                      step=step, \n","                      batch_size=batch_size)\n","val_gen = generator(float_data,\n","                    lookback=lookback,\n","                    delay=delay,\n","                    min_index=200001,\n","                    max_index=300000,\n","                    step=step,\n","                    batch_size=batch_size)\n","test_gen = generator(float_data,\n","                     lookback=lookback,\n","                     delay=delay,\n","                     min_index=300001,\n","                     max_index=None,\n","                     step=step,\n","                     batch_size=batch_size)\n","\n","# This is how many steps to draw from `val_gen`\n","# in order to see the whole validation set:\n","val_steps = (300000 - 200001 - lookback) // batch_size\n","\n","# This is how many steps to draw from `test_gen`\n","# in order to see the whole test set:\n","test_steps = (len(float_data) - 300001 - lookback) // batch_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"APgzTXL-goRK","colab_type":"text"},"cell_type":"markdown","source":["## A recurrent model\n","\n","\n","Instead of the `LSTM` layer introduced in the previous section, we will use the `GRU` layer, developed by Cho et al. in 2014. `GRU` layers \n","(which stands for \"gated recurrent unit\") work by leveraging the same principle as LSTM, but they are somewhat streamlined and thus cheaper \n","to run, albeit they may not have quite as much representational power as LSTM. This trade-off between computational expensiveness and \n","representational power is seen everywhere in machine learning.\n","\n","The model below uses recurrent dropout to help prevent overfitting.  "]},{"metadata":{"id":"0VKAkSyggoRZ","colab_type":"code","outputId":"ba7ba768-9423-453d-ce3f-e52698cc1a30","executionInfo":{"status":"ok","timestamp":1550638032918,"user_tz":300,"elapsed":3075858,"user":{"displayName":"Gregery T Buzzard","photoUrl":"https://lh6.googleusercontent.com/-wsbJkMRzkiI/AAAAAAAAAAI/AAAAAAAAADk/SiV0nNoXynw/s64/photo.jpg","userId":"14140499485883029775"}},"colab":{"base_uri":"https://localhost:8080/","height":1361}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras import layers, Input, Model\n","from keras.optimizers import RMSprop\n","\n","input_tensor = Input(shape=(None, float_data.shape[-1]))\n","x = layers.GRU(16,\n","               dropout=0.2,\n","               recurrent_dropout=0.2)(input_tensor)\n","x = layers.Dense(1)(x)\n","y = layers.Lambda(lambda x: x[:,-1,1] )(input_tensor)\n","output_tensor = layers.add([x,y])\n","model = Model(input_tensor, output_tensor)\n","#model = Sequential()\n","#model.add(layers.GRU(32,\n","#                     dropout=0.2,\n","#                     recurrent_dropout=0.2,\n","#                     input_shape=(None, float_data.shape[-1])))\n","#model.add(layers.Dense(1))\n","\n","model.compile(optimizer=RMSprop(), loss='mae')\n","history = model.fit_generator(train_gen,\n","                              steps_per_epoch=500,\n","                              epochs=40,\n","                              validation_data=val_gen,\n","                              validation_steps=val_steps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/40\n","500/500 [==============================] - 81s 161ms/step - loss: 0.3126 - val_loss: 0.2763\n","Epoch 2/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.3003 - val_loss: 0.2738\n","Epoch 3/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.2952 - val_loss: 0.2726\n","Epoch 4/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2933 - val_loss: 0.2705\n","Epoch 5/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2927 - val_loss: 0.2699\n","Epoch 6/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2908 - val_loss: 0.2677\n","Epoch 7/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2889 - val_loss: 0.2683\n","Epoch 8/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2881 - val_loss: 0.2698\n","Epoch 9/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2863 - val_loss: 0.2656\n","Epoch 10/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.2858 - val_loss: 0.2670\n","Epoch 11/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2855 - val_loss: 0.2657\n","Epoch 12/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2840 - val_loss: 0.2687\n","Epoch 13/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2830 - val_loss: 0.2655\n","Epoch 14/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2822 - val_loss: 0.2679\n","Epoch 15/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.2815 - val_loss: 0.2647\n","Epoch 16/40\n","500/500 [==============================] - 77s 155ms/step - loss: 0.2826 - val_loss: 0.2647\n","Epoch 17/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2819 - val_loss: 0.2653\n","Epoch 18/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2793 - val_loss: 0.2644\n","Epoch 19/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.2824 - val_loss: 0.2653\n","Epoch 20/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2801 - val_loss: 0.2633\n","Epoch 21/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2800 - val_loss: 0.2680\n","Epoch 22/40\n","500/500 [==============================] - 77s 155ms/step - loss: 0.2792 - val_loss: 0.2641\n","Epoch 23/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2784 - val_loss: 0.2673\n","Epoch 24/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2789 - val_loss: 0.2641\n","Epoch 25/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2783 - val_loss: 0.2612\n","Epoch 26/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2783 - val_loss: 0.2616\n","Epoch 27/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2785 - val_loss: 0.2631\n","Epoch 28/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2783 - val_loss: 0.2618\n","Epoch 29/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2771 - val_loss: 0.2605\n","Epoch 30/40\n","500/500 [==============================] - 76s 151ms/step - loss: 0.2768 - val_loss: 0.2619\n","Epoch 31/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.2755 - val_loss: 0.2607\n","Epoch 32/40\n","500/500 [==============================] - 76s 152ms/step - loss: 0.2764 - val_loss: 0.2618\n","Epoch 33/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.2756 - val_loss: 0.2626\n","Epoch 34/40\n","500/500 [==============================] - 78s 156ms/step - loss: 0.2739 - val_loss: 0.2623\n","Epoch 35/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2754 - val_loss: 0.2608\n","Epoch 36/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2743 - val_loss: 0.2597\n","Epoch 37/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2755 - val_loss: 0.2607\n","Epoch 38/40\n","500/500 [==============================] - 77s 153ms/step - loss: 0.2748 - val_loss: 0.2616\n","Epoch 39/40\n","500/500 [==============================] - 76s 153ms/step - loss: 0.2735 - val_loss: 0.2607\n","Epoch 40/40\n","500/500 [==============================] - 77s 154ms/step - loss: 0.2739 - val_loss: 0.2606\n"],"name":"stdout"}]},{"metadata":{"id":"ocgxmgoKgoRh","colab_type":"code","colab":{}},"cell_type":"code","source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]}]}