{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAUPA-HzQBRt"
   },
   "source": [
    "## Skip Gram Assignment\n",
    "\n",
    "Team member names:  Max Ruby, Carlos Salinas\n",
    "\n",
    "Complete all of the sections as described below.   Then run all, print to pdf using Chrome, and submit on Gradescope (indicating on your submission the start of each part of the assignment and choosing your team members).\n",
    "\n",
    "## TODO:\n",
    "Complete the programming tasks below.  Then answer these questions briefly.  \n",
    "\n",
    "Q1:  What are the differences in the untrained words similar to 'film' between the CBOW example and the Skip Gram example?  Do you think that's because of CBOW vs Skip Gram or the neural network architecture?\n",
    "\n",
    "Q2:  Why do you think the Skip Gram example produces so many words close to each target word?  How could you improve the performance of this example?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKcjlAyeQ3-j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbpCPjrwQBRv"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "V = 5000 # vocabulary size\n",
    "num_reviews = 5000 # number of reviews to use during training\n",
    "num_test = 100 # number of reviews to use during testing and validation\n",
    "dim = 20 # embedding dimension\n",
    "window_size = 2\n",
    "\n",
    "\n",
    "(train_data_full, train_labels_full), (test_data_full, test_labels_full) = imdb.load_data(num_words=V)\n",
    "train_data = train_data_full[0:num_reviews]\n",
    "test_data = test_data_full[0:num_test]\n",
    "val_data = test_data_full[num_test:2*num_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2R5D7xCWQBR0"
   },
   "source": [
    "\n",
    "The argument `num_words=V` means that we will only keep the top V most frequently occurring words in the training data. Rare words \n",
    "will be discarded. This allows us to work with vector data of manageable size.\n",
    "\n",
    "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
    "`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\".  The labels will not be used for this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_oOc6zEDlR8"
   },
   "source": [
    "Here's some code to decode back to English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O5lSPXpQBR9"
   },
   "outputs": [],
   "source": [
    "class WordIndexManager:\n",
    "  def __init__(self, word_index = []):\n",
    "    self.word_index = word_index\n",
    "    self.reverse_word_index = []\n",
    "    \n",
    "    if not (word_index == []): # Reverse the, mapping integer indices to words\n",
    "      self.reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "  def ind_to_string(self, word_ind):\n",
    "    # Decode a word; note that our indices were offset by 3\n",
    "    # because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "    return self.reverse_word_index.get(word_ind - 3, '?')\n",
    "\n",
    "  def inds_to_string(self, word_inds):  \n",
    "    # Put a list of decoded words into a string\n",
    "    decoded_review = ' '.join([self.ind_to_string(i) for i in word_inds])\n",
    "    return decoded_review\n",
    "  \n",
    "  def create_word_list(self, word_inds):\n",
    "    word_list = []\n",
    "    for ind in word_inds:\n",
    "      word_list.append(self.ind_to_string(ind))\n",
    "    return word_list\n",
    "\n",
    "# word_index is a dictionary mapping words to an integer index\n",
    "# We create an instance of a class to manage this index\n",
    "word_index = imdb.get_word_index()\n",
    "WIM = WordIndexManager(word_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_14EXWvN--5"
   },
   "source": [
    "Here are some examples of how to use the word index manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxjF_8R8N9MP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was ? with us all\n",
      "this\n"
     ]
    }
   ],
   "source": [
    "# print the first review\n",
    "print(WIM.inds_to_string(train_data[0]))\n",
    "\n",
    "# print the second word\n",
    "print(WIM.ind_to_string(train_data[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDXFjg4DQBSC"
   },
   "source": [
    "## TODO:\n",
    "Determine the maximum word length, and for each length from 1 to max, print the number of unique words of that length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnuHfHVNQBSE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length is 16\n",
      "Num of words of length 16 is 2\n",
      "Num of words of length 15 is 2\n",
      "Num of words of length 14 is 7\n",
      "Num of words of length 13 is 29\n",
      "Num of words of length 12 is 55\n",
      "Num of words of length 11 is 130\n",
      "Num of words of length 10 is 221\n",
      "Num of words of length 9 is 363\n",
      "Num of words of length 8 is 527\n",
      "Num of words of length 7 is 781\n",
      "Num of words of length 6 is 886\n",
      "Num of words of length 5 is 876\n",
      "Num of words of length 4 is 710\n",
      "Num of words of length 3 is 283\n",
      "Num of words of length 2 is 87\n",
      "Num of words of length 1 is 41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ** YOUR CODE HERE ** to define max_word_len and word_lens, which is an array \n",
    "# so that word_lens[i] is the length of the word with index i\n",
    "max_word_len = 0\n",
    "word_lens = np.zeros(V)\n",
    "for i in range(0,V):\n",
    "    word = WIM.ind_to_string(i)\n",
    "    word_lens[i] = len(word)\n",
    "    max_word_len = max(max_word_len,len(word))\n",
    "\n",
    "print('Max word length is ' + str(max_word_len))\n",
    "\n",
    "for i in range(max_word_len, 0, -1):\n",
    "    print('Num of words of length ' + str(i) + ' is ' + str(np.sum(word_lens == i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CujU_oDcRdi0"
   },
   "source": [
    "Determine the set of unique characters in the text. We add '?' to the list to allow for the '?' used in place of unknown words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8UOPn5cL7QB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', '\\x08', '\\x10', \"'\", '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x80', '\\x84', '\\x85', '\\x8d', '\\x8e', '\\x91', '\\x95', '\\x96', '\\x97', '\\x9a', '\\x9e', '\\xa0', '¡', '¢', '£', '¤', '¦', '§', '¨', '«', '\\xad', '®', '°', '³', '´', '·', 'º', '»', '½', '¾', '¿', 'À', 'Á', 'Ã', 'Ä', 'Å', 'È', 'É', 'Ê', 'Õ', 'Ø', 'Ü', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ō', '–', '‘', '’', '“', '”', '…', '₤', '\\uf0b7']\n"
     ]
    }
   ],
   "source": [
    "# List of unique characters in the corpus\n",
    "chars = set([])\n",
    "i = 0\n",
    "for (key, value) in word_index.items():\n",
    "  if not (set(key) <= chars):\n",
    "    chars = chars.union(list(set(key)))\n",
    "chars = ['?'] + sorted(list(chars))\n",
    "\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygvEGwKSSnpz"
   },
   "source": [
    "##TODO:\n",
    "Define a dictionary called char_index to map each character in the list above to  an index into new_chars (below) so that a-z are preserved, '?' is preserved, and all other characters are mapped to '*'. The output below should be \n",
    "```\n",
    "a\n",
    "z\n",
    "*\n",
    "?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNeaaRxiS65R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "z\n",
      "*\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "new_chars = ['.', '*', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# ** YOUR CODE HERE ** to define char_indices\n",
    "char_indices = dict()\n",
    "for char in chars:\n",
    "    char_indices[char] = 1\n",
    "\n",
    "char_indices['?'] = 2\n",
    "\n",
    "for i in range(97,97+26):\n",
    "    char_indices[chr(i)] = i +3-97\n",
    "\n",
    "print(new_chars[char_indices['a']])\n",
    "print(new_chars[char_indices['z']])\n",
    "print(new_chars[char_indices['\\x80']])\n",
    "print(new_chars[char_indices['?']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNttP6i-QBSY"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "\n",
    "We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. This is done with a data generator.  To use the data generator, call the function to get an instance of the generator, then iterate on that instance.  E.g.,\n",
    "\n",
    "```\n",
    "data_gen = generate_data(input_data, window_size, vocab_size, batch_size)\n",
    "for x,y in data_gen:\n",
    "    do something\n",
    "```\n",
    "\n",
    "##TODO:\n",
    "Fill in the code below to build one-hot letter by letter encodings from word indices and to generate training batches.  \n",
    "\n",
    "For build_words, each word index produces an array of size (max_word_len, len(new_chars)), where each vector [i,:] is a one-hot encoding of the ith letter (starting from index 0). \n",
    "\n",
    "For generate_data, each batch should consist of \n",
    "\n",
    "```\n",
    "x: (batch_size,  max_word_len, len(new_chars))\n",
    "y: (batch_size, V)\n",
    "```\n",
    "x can be constructed from build_words using the indices for the words at the center of each sample. y is a probability distribution over the vocabulary using the context words within window_size before and after the central word for a given sample.  \n",
    "\n",
    "E.g., if the indices in a passage are 45, 13, 98, 6, 9, 18, then one sample would construct x[0,:,:] from 98, and y[0,:] would have entries 0.25 in each of indices 45, 13, 6, 9.  The next sample would construct x[1,:,:] from 6, and y[1,:] would have entries 0.25 in each of indices 13, 98, 9, 18.  If a context word is repeated, then it gets proportionally more weight.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcRFyx5HQBSa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "# Take in a list of word indices\n",
    "# Convert each word to an array of size (max_word_len, len(new_chars))\n",
    "# Use one-hot encoding on each letter and pad with 0s to fill up max_word_len\n",
    "def build_words(word_inds, max_word_len=max_word_len, char_list=new_chars, char_indices=char_indices):\n",
    "    output = np.zeros((len(word_inds), max_word_len, len(char_list)), dtype=np.bool)\n",
    "    # ** YOUR CODE HERE ** to build the output    \n",
    "    words = WIM.inds_to_string(word_inds)\n",
    "    for i in range(len(word_inds)):\n",
    "        word_len = len(words[i])\n",
    "        for j in range(word_len):\n",
    "            index = char_indices[words[i][j]]\n",
    "            output[i][j][index] = 1\n",
    "    return output\n",
    "      \n",
    "# Print the words generated from build_words  \n",
    "def print_words(center_words, char_map=new_chars):\n",
    "  for word in center_words:\n",
    "    for i in range(word.shape[0]):\n",
    "      cur_ind = np.argwhere(word[i,:] > 0)  # Find the index of the current letter\n",
    "      if len(cur_ind) > 0:\n",
    "        print(char_map[cur_ind[0][0]], end='')  # Print this letter using the character map\n",
    "      else: \n",
    "        print('')\n",
    "        break\n",
    "    \n",
    "# Generate training samples:  \n",
    "# Training input is batch_size x max_word_len x len(new_chars)\n",
    "# Each element in a batch is encoded using build_words - one hot encoding of letters in a word\n",
    "# Each label is a probability distribution on the vocabulary: equal weighting for each context word\n",
    "def generate_data(corpus, window_size, V, batch_size=16):\n",
    "    maxlen = window_size*2\n",
    "    # ** YOUR CODE HERE ** to generate batches\n",
    "    while 1:\n",
    "        for i in range(0,num_reviews):\n",
    "            review_size = len(corpus[i])\n",
    "            total_batch_num = (review_size - 2*window_size)//batch_size\n",
    "            for current_batch_num in range(total_batch_num):\n",
    "                x = np.zeros((batch_size, max_word_len, len(new_chars)))\n",
    "                y = np.zeros((batch_size, V))\n",
    "                for batch_val in range(batch_size):\n",
    "                    y[batch_val,corpus[i][batch_val + current_batch_num*batch_size]] = 1\n",
    "                    for k in range(2*window_size):\n",
    "                        if k < window_size:\n",
    "                            x[batch_val, k, corpus[i][current_batch_num*batch_size + batch_val + k - window_size]] = 1\n",
    "                        else:\n",
    "                            x[batch_val, k, corpus[i][current_batch_num*batch_size + batch_val + k + 1 - window_size]] = 1\n",
    "                yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SN7gY-bdJT-"
   },
   "source": [
    "Test your generator.  The final line of context words should be \n",
    "\n",
    "\n",
    "```\n",
    "the 0.25 they 0.25 played 0.25 suited 0.25\n",
    "```\n",
    "and the final line of center words should be\n",
    "\n",
    "\n",
    "```\n",
    "part\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2w4Q_LznVsL"
   },
   "outputs": [],
   "source": [
    "\n",
    "window_size = 2\n",
    "train_gen = generate_data(train_data, window_size, V)\n",
    "val_gen = generate_data(val_data, window_size, V)\n",
    "test_gen = generate_data(test_data, window_size, V)\n",
    "\n",
    "for center_words, contexts in train_gen:\n",
    "  print('** Context words **')\n",
    "  for vector in contexts:\n",
    "    inds = np.argwhere(vector > 0)\n",
    "    for j in range(inds.shape[0]):\n",
    "      cur_ind = inds[:,0][j]\n",
    "      print(WIM.ind_to_string(cur_ind) + ' ' + str(vector[cur_ind]) + ' ' , end='')\n",
    "    print('')\n",
    "  print('\\n** Center words **')\n",
    "  print_words(center_words)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHX3tZsHQBSe"
   },
   "source": [
    "##TODO:\n",
    "Construct the model in two stages.  First construct an LSTM layer called word_embedding.  This should use an embedding dimension of dim and take training samples from your generator.\n",
    "\n",
    "Then create the skip_gram model by using the word_embedding model followed by a dense layer with softmax activation onto the vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkjxHTsQUHwS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ** YOUR CODE HERE ** for word_embedding\n",
    "\n",
    "word_embedding.summary()\n",
    "\n",
    "# ** YOUR CODE HERE ** for skip_gram\n",
    "\n",
    "skip_gram.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "skip_gram.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGTTFKGSeapS"
   },
   "source": [
    "Here we define a function to save the embedding weights.  In this case we need some separate code to get the weights from the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fL7OSQI4t71"
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_weights(weights, vocab_size=V, dim=dim, filename='vectorsSG.txt'):\n",
    "  f = open(filename ,'w')\n",
    "  f.write('{} {}\\n'.format(vocab_size-1, dim))\n",
    "  for i in range(1,vocab_size):\n",
    "      str_vec = ' '.join(map(str, list(weights[i, :])))\n",
    "      word = WIM.ind_to_string(i)\n",
    "      f.write('{} {}\\n'.format(word, str_vec))\n",
    "  f.close()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xucFOxEen5G"
   },
   "source": [
    "Here we use the LSTM to get the embedding for all of the words in the vocabulary and then save the corresponding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u69CeeGgz5a-"
   },
   "outputs": [],
   "source": [
    "# Construct word embedding dictionary\n",
    "def get_weights(inds):\n",
    "  cur_words = build_words(inds)\n",
    "  return word_embedding.predict(cur_words)\n",
    "\n",
    "\n",
    "weightsUT = get_weights(range(V))\n",
    "print(weightsUT.shape)\n",
    "save_weights(weightsUT, filename='untrainedSG.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mF3AvOPae5PO"
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmLgYkF2zzdI"
   },
   "outputs": [],
   "source": [
    "\n",
    "val_steps = 100\n",
    "\n",
    "history = skip_gram.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1500,\n",
    "                              epochs=6,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFr8K9SLfn3j"
   },
   "source": [
    "Plot the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTrOgcLiqUlT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TS7Ww0MHQBSl"
   },
   "source": [
    "Save the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iwo7N4945ou1"
   },
   "outputs": [],
   "source": [
    "all_words = build_words(range(V))\n",
    "weights = word_embedding.predict(all_words)\n",
    "print(weights.shape)\n",
    "save_weights(weights, filename='vectorsSG.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrPNToKzfz1e"
   },
   "source": [
    "Load the word embeddings and compare trained and untrained embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2dmpE7QiNtL"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "w2vUT = gensim.models.KeyedVectors.load_word2vec_format('./untrainedSG.txt', binary=False)\n",
    "w2vT = gensim.models.KeyedVectors.load_word2vec_format('./vectorsSG.txt', binary=False)\n",
    "\n",
    "def print_similarities(word, w2vUT=w2vUT, w2vT=w2vT):\n",
    "  print('Nearest words and similarities to \"' + word + '\" ')\n",
    "  print('Untrained similarities\\tTrained similarities\\n')\n",
    "  for item1, item2 in zip(w2vUT.most_similar(positive=[word]), w2vT.most_similar(positive=[word])):\n",
    "    print(\"{:10s}\".format(item1[0]) + ', ' + \"{:.2f}\".format(item1[1]) + '\\t' \n",
    "          + \"{:10s}\".format(item2[0]) + ', ' + \"{:.2f}\".format(item2[1]))\n",
    "  print(' ')\n",
    "\n",
    "print_similarities('movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2Z9le_MGLKL"
   },
   "outputs": [],
   "source": [
    "print_similarities('film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3I7toQT3-6e"
   },
   "outputs": [],
   "source": [
    "print_similarities('role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ll2BIvIgOoP"
   },
   "outputs": [],
   "source": [
    "print('Word pair similarity')\n",
    "print('\\t\\t\\tUntrained\\tTrained')\n",
    "print('film and movie: \\t' + \"{:.2f}\".format(w2vUT.similarity('film', 'movie')) \n",
    "      + '\\t\\t' + \"{:.2f}\".format(w2vT.similarity('film', 'movie')))\n",
    "print('man and woman:   \\t' + \"{:.2f}\".format(w2vUT.similarity('man', 'woman')) \n",
    "      + '\\t\\t' + \"{:.2f}\".format(w2vT.similarity('man', 'woman')))\n",
    "print('plot and talent: \\t' + \"{:.2f}\".format(w2vUT.similarity('plot', 'talent')) \n",
    "      + '\\t\\t' + \"{:.2f}\".format(w2vT.similarity('plot', 'talent')))\n",
    "\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_u_-OrC_nYV"
   },
   "source": [
    "Use TSNE to plot the primary two components of the embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IdGEAGF7aEK"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "number_of_words = 1000\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(weights[0:number_of_words])\n",
    "word_list = WIM.create_word_list(range(number_of_words))\n",
    "\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = X_embedded[0:number_of_words,0], \n",
    "    y = X_embedded[0:number_of_words, 1],\n",
    "    mode = 'markers',\n",
    "    text= word_list[0:number_of_words]\n",
    ")\n",
    "\n",
    "layout = dict(title= 'Trained t-SNE 1 vs t-SNE 2 for first 1000 words ',\n",
    "              yaxis = dict(title='t-SNE 2'),\n",
    "              xaxis = dict(title='t-SNE 1'),\n",
    "              hovermode= 'closest')\n",
    "\n",
    "fig = dict(data = [trace], layout= layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sm5tjUvh9rKH"
   },
   "outputs": [],
   "source": [
    "def configure_plotly_browser_state():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "  \n",
    "configure_plotly_browser_state()  \n",
    "\n",
    "py.init_notebook_mode()\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jttjSQqG_9Vm"
   },
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(weightsUT[0:number_of_words])\n",
    "word_list = WIM.create_word_list(range(number_of_words))\n",
    "\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = X_embedded[0:number_of_words,0], \n",
    "    y = X_embedded[0:number_of_words, 1],\n",
    "    mode = 'markers',\n",
    "    text= word_list[0:number_of_words]\n",
    ")\n",
    "\n",
    "layout = dict(title= 'Untrained t-SNE 1 vs t-SNE 2 for first 1000 words ',\n",
    "              yaxis = dict(title='t-SNE 2'),\n",
    "              xaxis = dict(title='t-SNE 1'),\n",
    "              hovermode= 'closest')\n",
    "\n",
    "fig = dict(data = [trace], layout= layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VArWZISZAE54"
   },
   "outputs": [],
   "source": [
    "configure_plotly_browser_state()  \n",
    "\n",
    "py.init_notebook_mode()\n",
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of SkipGram_template.ipynb",
   "provenance": [
    {
     "file_id": "12sd0y5r6k-iBtd4pddovQgRL_XCnaWuN",
     "timestamp": 1552062104787
    },
    {
     "file_id": "1AvCuBGQ8PpCxOOP9G2JeWStN5Z9S4BjX",
     "timestamp": 1552060880695
    },
    {
     "file_id": "17tPgWRj2GUfHyBv5CinrbDG5KEk5oMlm",
     "timestamp": 1551488342397
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
