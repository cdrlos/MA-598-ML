\noindent \textbf{Title of paper:} ``'Explaining and Harnessing Adversarial
Examples'' by Goodfellow, Shlens, and Szegedy. 

\noindent\textbf{What is their primary result?}

\noindent\textbf{Why is this important?}

\noindent\textbf{What are their key ideas?}

\noindent\textbf{What are the limitations, either in performance or applicability?}

\noindent\textbf{What might be an interesting next step based on this work?}

\noindent\textbf{What's the architecture?}

\noindent\textbf{How did they train and evaluate it?}

\noindent\textbf{Did they implement something?}

Adversarial examples are inputs to a neural network that result in an incorrect
output from the network.

Why this happens? Early attemps at explaining this problem focus on nonlinearity
and overfitting and incufficient regularization.

Hypothesize: Neural networks are too linear to resist linear adversarial
perturbation.

LSTMs, ReLUs , and maxout networks are all intentionally designated to behave in
very linear ways, so that they are easier to optimize.

Nonlinear models suchh as sigmoid networks are carefully.

Introduction: Some previous work on adversarial examples.

That is misclasisfy examlpes that are slightly different from correctly
classified examples.
A wid variety of models with different architecture trainde

Open question.

Why do adversanrial examples generalize?

One adversarial example can attack multiple aversarial networks.




%%% Local Variables:
%%% TeX-master: "../MA598-DL-HW"
%%% End: