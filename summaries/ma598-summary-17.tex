\noindent \textbf{Title of paper:} ``Understanding Deep Convolutional Networks''
by Mallat.

\noindent\textbf{What is their primary result?} The paper introduces a
mathematical framework to analyze contraction and separation properties of deep
convolutional networks. 

\noindent\textbf{Why is this important?} We want to understand why convolutional
networks are so powerful. This paper is an important step in our understanding. 

\noindent\textbf{What are their key ideas?} The author introduces several
important concepts to the analysis of convolutional networks. Key among these
are the following.
\begin{itemize}[noitemsep]
\item In high dimensions, $x$ has a considerable number of parameters; this is a
  dimensionality curse. To remedy the situation, one might want to introduce a
  new variable $\Phi(x)$ where $\Phi$ is a contractive operator which reduces
  the range of variations of $x$, while still separating different values of
  $f$, i.e., whenever $\Phi(x)\neq\Phi(x')$ whenever $f(x)\neq f(x')$.
\item Once we have less variability on $x$ a low-dimensional linear projection
  on $\Phi(x)$ in the direction of a high-dimensional linear space where $f$
  remains nearly constant reduces the actual dimension of the problem.
\item What deep neural networks actually do is contract the space and linearize
  transformations $\Phi$ along which $f$ remains nearly constant. These
  directions are defined by linear operators which belong to groups of local
  symmetries.
\item The author analyzes the problem of linearizing the action of
  hi-dimensional groups of operators, beginning with the group of translations
  and diffeomorphisms.
\item At this point, the author introduces the wavelet transform to deal with
  the problem of preserving separability while linearizing the diffeomorphisms
  (i.e., the smooth variable transformations $\Phi$).
\item General deep networks architectures iterate on linear operators which
  filter and linearly combine different channels in the network layers. This is
  followed up by contractive nonlinearities.
\item The author then analyzes how nonlinear contractions interact with linear
  operators. For this he introduces simpler architecture which defines a
  nonlinear scattering transform for which sparsity is necessary for effective
  image reconstruction.
\item Finally, the author extends these ideas to multiscale hierarchical
  convolutional networks. 
\end{itemize}

% \noindent\textbf{What are the limitations, either in performance or applicability?}

\noindent\textbf{What might be an interesting next step based on this work?} The
author points out than we need complexity measures, approximation theorems in
high-dimensional function spaces, and convergence of filter optimization to
fully understand the mathematics of convolutional networks. Presumably this can
be a direction of further research on this topic. 

% \noindent\textbf{What's the architecture?}

% \noindent\textbf{How did they train and evaluate it?}

% \noindent\textbf{Did they implement something?}

%%% Local Variables:
%%% TeX-master: "../MA598-DL-HW"
%%% End: