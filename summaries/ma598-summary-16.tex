\noindent \textbf{Title of paper:} ``Learning to Optimize
Neural Nets'' by Li and Malik.

\noindent\textbf{What is their primary result?} The authors
show that a recently proposed framework for

\noindent\textbf{Why is this important?} The paper tackles a difficult
problem; that of designing machine learning algorithms through the use
of machine learning.

\noindent\textbf{What are their key ideas?}

\noindent\textbf{What are the limitations, either in performance or applicability?}

\noindent\textbf{What might be an interesting next step based on this work?}

\noindent\textbf{What's the architecture?}

\noindent\textbf{How did they train and evaluate it?}

\noindent\textbf{Did they implement something?}

Learning to optimize is a recently proposed framework for learning. In
this paper we explore learning an optimization algorithm for training
shallow neural networks. Such high-dimensional stochastic optimization
problems present interesting challenges for existing reinforcement
learning algorithms. We develop an extension in this setting and
demonstrate that the learned optimization algorithm consistently
outperforms other known optimization algorithms even on unseen tasks
and is robust to changes in stochasticity of gradients and the neural
net architecture. More specifically, we show that an optimization
algorithm with the proposed method on the problem of training a neural
net on MNIST generalizes to the problem of training neural nets on the
Toronto Faces Dataset, CIFAR-10, and CIFAR-100.

%%% Local Variables:
%%% TeX-master: "../MA598-DL-HW"
%%% End:
