\noindent \textbf{\large Title of paper: } Attention is All You Need by
Polosukhin et al.

\noindent\textbf{\large What is their primary result?} The authors propose a
novel network architecture tiled ``the Transformer,'' which is based solely on
attention mechanisms, and dispensing with recurrence and convolutions found in
the more prolific sequence transduction models.

\noindent\textbf{\large Why is this important?} The authors provide experimental
data suggesting that this model is superior in quality, more parallelizable, and
requires less training time.

\noindent\textbf{\large What are their key ideas?} The Transformer follows the
overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, but

\noindent\textbf{\large What are the limitations, either in performance or
  applicability?}

\noindent\textbf{\large What might be an interesting next step based on this
  work?}

\noindent\textbf{\large What's the architecture?}

\noindent\textbf{\large How did they train and evaluate it?}

\noindent\textbf{\large Did they implement something?}

%%% Local Variables:
%%% TeX-master: "../MA598-DL-HW"
%%% End: