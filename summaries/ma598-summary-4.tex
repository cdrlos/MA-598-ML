\section{Deep Residual Learning for Image Recognition}
\subsection*{What is their primary result?}
The authors present a residual learning framework whose purpose is to ease the
training of deep networks. 

\subsection*{Why is this important?}

\subsection*{What are their key ideas?}

\subsection*{What are the limitations, either in performance or applicability?}

\subsection*{What might be an interesting next step based on this work?}

\subsection*{What's the architecture?}

\subsection*{How did they train and evaluate it?}

\subsection*{Did they implement something?}


% This was actually talk four

% \subsection*{Introduction}
% Deep nerual networks have become better at classification tasks with increasingc
% number of layers.

% Is getting better as easy as adding more layers?

% No. Why? Overfitting, exploding/vanishing gradient, and degradation.

% \subsection*{Constructed network thought experiment}

% Let $S_1$ be a shallow neural network that maps inputs.

% Hypothesis: If instead we use a block which mimics the identity map, then the
% residual is easy to train.

% Let $H(x)=\calF(x)+x$. Then the output of the residual block is $\sigma(H(x))$
% and
% \begin{align*}
%   \sigma(H(x)) &= \sigma(\calF(x)+x)\\
%                &=\sigma(W_2\sigma(W_1x)+x)\\
%                &=\sigma(x)=x,
% \end{align*}
% when $W_2=0$$.

% More generally
% \[
%   H(x)=\calF(x)+W_sx.
% \]

%%% Local Variables:
%%% TeX-master: "../MA598-DL-HW"
%%% End: